---
title: "TopicModeling-NINR"
author: "Mina Narayanan"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
## Global knitr options
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(autodep = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

```

```{r, warning = FALSE, message = FALSE}
## Increase Memory Limit
invisible(utils::memory.limit(64000))
options(java.parameters = "-Xmx64000m")

## Data Frame Packages
library(dplyr)
library(stringr)
library(readr)
library(readxl)

## Data Visualization Packages
library(ggplot2)
library(Rtsne)
library(scales)

## Text Mining Packages
library(data.table)
library(Matrix)
library(text2vec)
library(tm)
library(SnowballC)
library(rARPACK)
library(ggupset)
```

## Load Data

First we load the data of NINR grants awarded in 2019.  We pull four sources of text:
\begin{itemize}
\item T - Title
\item A - Abstract Text
\item S - Specific Aims (SA) Text
\end{itemize}x
```{r, message = FALSE, cache = TRUE}
ninr_1 = read_excel("./../data/NINR_Translational.xlsx", guess_max = 1000000)
ninr_2 = read_excel("./../data/NIA_Translational.xlsx", guess_max = 1000000)

# randomly sample so that each institute has an equal number of grants
smaller_count <- -1

if (nrow(ninr_1) < nrow(ninr_2)) {
  smaller_count <- nrow(ninr_1)
  # randomly sample smaller_count rows from ninr_2
  ninr_2 <- ninr_2[sample(nrow(ninr_2), smaller_count), ]
} else {
  smaller_count <- nrow(ninr_2)
  # randomly sample smaller_count rows from ninr_1
  ninr_1 <- ninr_1[sample(nrow(ninr_1), smaller_count), ]
}



ninr = rbind(ninr_1, ninr_2)



#nia = read_excel("./../data/NIA_Translational.xlsx", guess_max = 1000000)

# TODO: keep PCC in ninr dataframe in the future


```

## Rename Columns
```{r}
rename <- function(institute) {
  names(institute)[names(institute) == 'PI Name'] <- 'PI.Name'
  names(institute)[names(institute) == 'PO Name'] <- 'PO.Name'
  names(institute)[names(institute) == 'RFA/PA Number'] <- 'RFA.PA.Number'                                                                                 
  names(institute)[names(institute) == 'Stat Desc'] <- 'Stat.Desc'                                                                                          
  names(institute)[names(institute) == 'Abstract Text (only)'] <- 'Abstract' 
  names(institute)[names(institute) == 'SA Text'] <- 'SA.Text' 
  return(institute)
}

ninr <- rename(ninr)
#nia <- rename(nia)

```

Let's restrict to R01 equivalents

```{r}
#r01 = c("DP1", "DP2", "DP5", "R01", "R35", "R37", "R56", "RF1", "RL1", "U01")
```

## Text Variable

Let's create a text variable.
```{r}
text_field <- function(institute) {
  text = c("Title", "Abstract", "SA.Text")
  institute <- institute[institute$Abstract != "-",]
  institute$Text <- institute[[text[1]]]

  if(length(text) > 1){
    for(i in 2:length(text)){
      institute$Text <- paste(institute$Text, institute[[text[i]]])
      print(i)
    }
  }
  print(substr(institute[1, "Text"], 1000, 11000))

  institute = select(institute, -c("Title", "Abstract", "SA.Text"))
  institute = mutate(institute, Text = ifelse(is.na(Text), "", Text))
  institute = filter(institute, Text != "")

  cat("Sections used:", text, "\n\n")
  return(institute)
}

ninr <- text_field(ninr)
#nia <- text_field(nia)

```

### Clean Up Text

Let's clean up the text by making everything lowercase, removing punctuation, etc.
```{r}

clean_text <- function(institute) {
  # Remove weird characters
  institute$Text = str_replace_all(institute$Text, "\n", " ")
  institute$Text = str_replace_all(institute$Text, "\r", " ")
  institute$Text = str_replace_all(institute$Text, "'", "")
  institute$Text = str_replace_all(institute$Text, "-", " ")

  # Replace non-alpha numeric characters with a space
  institute$Text = str_replace_all(institute$Text, "[^abcdefghijklmnopqrstuvwxyzABCDEFHIJKLMNOPQRSTUVWXZ]", " ") #0123456789 ]", " ")

  # Put everything in lowercase
  institute$Text = tolower(institute$Text)

  # Remove a few select words and extra spaces
  institute$Text = str_replace_all(institute$Text, "abstract", " ")
  institute$Text = str_replace_all(institute$Text, "project", " ")
  institute$Text = str_replace_all(institute$Text, "proposal", " ")
  institute$Text = str_replace_all(institute$Text, "summary", " ")
  institute$Text = str_replace_all(institute$Text, "narrative", " ")
  institute$Text = str_replace_all(institute$Text, "background", " ")
  institute$Text = str_replace_all(institute$Text, "supplement", " ")
  institute$Text = str_replace_all(institute$Text, "significance", " ")
  institute$Text = str_replace_all(institute$Text, "overall", " ")
  institute$Text = str_replace_all(institute$Text, "title", " ")
  institute$Text = str_replace_all(institute$Text, "specific aims", " ")
  institute$Text = str_replace_all(institute$Text, "goal", " ")
  institute$Text = str_replace_all(institute$Text, "research plan", " ")
  institute$Text = str_replace_all(institute$Text, "overview", " ")
  institute$Text = str_replace_all(institute$Text, "\\s+", " ")
  institute$Text = str_replace_all(institute$Text, "description provided by applicant", " ")

  institute$Text = str_trim(institute$Text, side = "both")
  
  return(institute)
}


#nia <- clean_text(nia)
ninr <- clean_text(ninr)

```

### Unique Text

Let's only keep applications with unique text.
```{r}
remove_duplicates <- function(institute) {
  institute <- institute[!duplicated(institute[ , c("Text")]),]
  return(institute)
}
#nia <- remove_duplicates(nia)
ninr <- remove_duplicates(ninr)

```


## DTM

Let's build a Document-Term Matrix (DTM).
```{r}
#build_dtm <- function(institute) {
  setDT(ninr)
  setkey(ninr, Project)
#  return(institute)
#}

#nia <- build_dtm(nia)
#ninr <- build_dtm(ninr)

```


###  Vocabulary

Let's build a vocabulary that uses stemming.
```{r}

build_vocab <- function(institute) {
  stem_tokenizer = function(x) {
    word_tokenizer(x) %>% lapply(SnowballC::wordStem, language="en") 
  }

  it = itoken(institute$Text,
            tokenizer = stem_tokenizer,
            ids = institute$Project, 
            progressbar = FALSE)

  stop = stopwords(kind = "SMART")
  stop = str_replace_all(stop, "'", "")

  vocab = create_vocabulary(it, stopwords = stop, ngram = c(1L, 2L))

  cat("Number of terms in vocabulary:", dim(vocab)[1])
  #vocab %>% arrange(desc(term_count)) %>% head(20)
  
  vocab = prune_vocabulary(vocab,
                                term_count_min = 10)
                                #doc_count_min = 5,
                                #doc_proportion_max = 0.9,
                                #doc_proportion_min = 0.001)

  cat("Number of terms in vocabulary:", dim(vocab)[1])
  
  # TODO: Create list of custom stopwords
  paul1 = read_excel("B1_Round1_2019-12-06.xlsx", sheet = "Stopwords")
  paul2 = read_excel("B1_Round2_2019-12-06.xlsx", sheet = "Stopwords")
  paul = rbind(paul1, paul2)
  paul = paul$Stopword
  #print(paul)

  vocab = filter(vocab, !(term %in% paul))
  #cat("\nNumber of terms in vocabulary:", dim(vocab)[1])

  vectorizer = vocab_vectorizer(vocab)
  dtm = create_dtm(it, vectorizer)

  #identical(rownames(dtm), as.character(nih$AID))

  cat("DTM dimensions:", dim(dtm))
  
  return(dtm)
}

#nia_dtm <- build_vocab(nia)
ninr_dtm <- build_vocab(ninr)

```

### Write DTM to file

```{r}
#nia_dtm_matrix <- as.matrix(nia_dtm)
ninr_dtm_matrix <- as.matrix(ninr_dtm)
```

## Topic Modeling

Here we use the most popular topic modeling algorithm, Latent Dirichlet Allocation (LDA).  LDA assigns a probability to each word of belonging to each topic, and then assigns a percentage of each topic to each document.  For example, if we had 2 topics, LDA would assign two percentages (which add up to 100%) to each document.  Here we plot the 20 most indicative words of each topic, and the distribution of the most likely topics of the documents.

### Lowest Perplexity

UNCOMMENT THIS CODE TO FIND THE OPTIMAL PARAMETERS AGAIN
```{r}
opt_param <- function(dtm, filename) {
  
 as = c(0.05, 10^(-5:5))
 bs = c(0.05, 10^(-5:5))
 topics = 12:13
 
 pp = NULL
 
 for(topic in topics){
   a1 = 50/topic # Default Values
   b1 = 1/topic  # Default Values
   lda_model = LDA$new(doc_topic_prior = a1, 
                       topic_word_prior = b1,
                       n_topics = topic)
   
   set.seed(123)
   doc_topic_distr = 
        lda_model$fit_transform(x = dtm, 
                                n_iter = 1000, 
                                convergence_tol = 0.0001, 
                                n_check_convergence = 25, 
                                progressbar = FALSE,
                                verbose = FALSE)
   
   topic_word_distr = lda_model$topic_word_distribution
  
   p1 = perplexity(dtm, topic_word_distr, doc_topic_distr)
   
   print(paste(a1, b1, p1))
   
   for(a in as){
     for(b in bs){
       lda_model = LDA$new(doc_topic_prior = a, 
                           topic_word_prior = b,
                           n_topics = topic)
       
       set.seed(123)
       doc_topic_distr = 
            lda_model$fit_transform(x = dtm, 
                                    n_iter = 1000, 
                                    convergence_tol = 0.0001, 
                                    n_check_convergence = 25, 
                                    progressbar = FALSE,
                                    verbose = FALSE)
       
       topic_word_distr = lda_model$topic_word_distribution
       
       p = perplexity(dtm, topic_word_distr, doc_topic_distr)
       # TODO: Check coherence
       
       print(paste(topic, a, b, p))
       
       if(p < p1){
         a1 = a
         b1 = b
         p1 = p
       }
     }
   }
   
   pp1 = data.frame(Topics = topic, Alpha = a1, Beta = b1, Perplexity = p1)
   pp = rbind(pp, pp1)
 }
 
 cat("\n\n")
 
 pp
 write_csv(pp, filename)
   
}

#opt_param(nia_dtm, "LDA_Opt_Param_NICHD_Prevention.csv")
opt_param(ninr_dtm, "LDA_Opt_Param_NINR_NIA_Translational.csv")

```

Let's load the optimal parameters.
```{r}
pp_ninr = read_csv("LDA_Opt_Param_NINR_NIA_Translational.csv")
pp_ninr = data.frame(pp_ninr)

#pp_nia = read_csv("LDA_Opt_Param_NIA_Translational.csv")
#pp_nia = data.frame(pp_nia)
```

Now we choose the desired number of topic.
```{r, message = FALSE, warining = FALSE}
#lda_run <- function(pp) {
  topic = 13

  pp1 = filter(pp_ninr, Topics == topic)
  a1 = pp1$Alpha
  b1 = pp1$Beta
  t1 = pp1$Topics

  lda_model = LDA$new(doc_topic_prior = a1, 
                    topic_word_prior = b1,
                    n_topics = t1)

  set.seed(123)
  doc_topic_distr = 
     lda_model$fit_transform(x = ninr_dtm, 
                             n_iter = 1000, 
                             convergence_tol = 0.0001, 
                             n_check_convergence = 25, 
                             progressbar = FALSE,
                             verbose = FALSE)

  topic_word_distr = lda_model$topic_word_distribution


  #cat("\n\n")

  d = as.matrix(doc_topic_distr)
  d = data.frame(Project = row.names(d), d)
  colnames(d) = str_replace_all(names(d), "X", "T")

  for(i in 1:dim(d)[1]){
      temp = names(sort(d[i, 2:dim(d)[2]], decreasing = TRUE))[1:2]           
      d[i, "Topic1"] = temp[1]
      d[i, "Topic2"] = temp[2]
  }

  #d$Topic = factor(d$Topic)

  l = dim(d)[2]
  d = d[, c(1, l-1,l, 4:l-2)]

  cat("\n\n")

  k = lda_model$get_top_words(n = 20, topic_number = 1:t1, lambda = 0.25)

  k

  #cat("\n\n")

  #perplexity(dtm, topic_word_distr, doc_topic_distr)

  #cat("\n")
  table(d$Topic1)

  k1 = data.frame(k)
  colnames(k1) = paste0("Topic", 1:t1)

  str(k1)

#  return(k)
#}

#k_nia <- lda_run(pp_nia)
#k_ninr <- lda_run(pp_ninr)
```
# LDAvis
```{r}
topics = 13
dir.create(paste0("LDAvis_", topics))
lda_model$plot(out.dir = paste0("LDAvis_", topics), open.browser = TRUE)

```

## Topic Order

The topic order used by LDAvis is determined by the default circle area, $A_k$, described in the following PDF: https://rdrr.io/cran/LDAvis/f/inst/doc/details.pdf.  We can get the topic ordering directly from the JSON file that was created by LDAvis and reorder our topics.
```{r}
j = read_json(paste0("LDAvis_", topics, "/lda.json"))
j = j$topic.order
j = as.integer(j)


k = lda_model$get_top_words(n = 20, topic_number = 1:topics, lambda = 0.3)
k = k[, j]

k1 = data.frame(k1)
colnames(k1) = paste0("T", 1:topics)
k1
```
# -----------------IGNORE START------------------------------------------------------------------------------------------------------------

# Derive Acronyms Using Lookup Table
```{r}
acronym_lookup <- data.frame("Acronym" = c("pa", "smm", "hf", "plwh", "t2dm", "msm", "fsw", "hana", "acp", "cds", "cqa", "cci", "comt", "bpsd", "pwd", "fc", "eolpc", "pcrc", "cmc", "4_bpsd", "pes", "mcc",
                                           "hhc", "dts", "cri", "vte", "pru", "cbpr", "cbt", "trkb", "lbp", "trkb_t1", "oa", "eol"), 
                             "Derivation" = c("physical activity", "severe maternal morbidity ", "heart failure", "persons living with HIV", "type 2 diabetes mellitus", "men who have sex with men", "female
                                              sex worker", "HIV-associated non-AIDS conditions", "advance care planning", "clinical decision support", "communication quality analysis", "chronic critical
                                              illness", "catechol-O-methyltransferase", "behavioral and psychological symptoms of dementia", "persons with dementia", "fecal calprotectin", "end of life and
                                              palliative care", "Palliative Care Research Consortium", "children with medical complexity", "behavioral and psychological symptoms of dementia", "patient
                                              engagement specialist", "multiple chronic conditions", "home healthcare", "disinfection tracking system", "cardiorespiratory instability", "Venous
                                              thromboembolism", "pruritis", "community-based participatory research", "cognitive behavioral therapy", "tropomyosin-related receptor kinase type B", "lower
                                              back pain", "truncated isoform of Trkb", "osteoarthritis", "end of life"))                                                                                                                                                         
  
  
for (row in 1:nrow(k)) {
  for (col in 1:ncol(k)) {
    for (row_acro in 1:nrow(acronym_lookup)) {
      if (as.String(k[row, col]) == as.String(acronym_lookup[row_acro, 1])) {
        k[row, col] <- as.String(acronym_lookup[row_acro, 2])
      }
    }
  }
}

k
```

### Check Institute Overlap
```{r}
k_ninr_keys <- list()
k_nia_keys <- list()
index <- 1

for (row in 1:nrow(k_ninr)) {
  for (col in 1:ncol(k_ninr)) {
    for (row_other in 1:nrow(k_nia)) {
      for (col_other in 1:ncol(k_nia)) {
        ninr_word <- as.String(k_ninr[row, col])
        nia_word <- as.String(k_nia[row_other, col_other])
        if ((grepl(tolower(ninr_word), tolower(nia_word), fixed=TRUE) | 
            grepl(tolower(nia_word), tolower(ninr_word), fixed=TRUE)) && 
            (!(ninr_word %in% c("eh", "sm", "sca", "eh", "af", "ic", "asp"))) && (!(nia_word %in% c("eh", "sm", "sca", "eh", "af", "ic", "asp")))) {
              k_ninr_keys[index] <- ninr_word
              k_nia_keys[index] <- nia_word
              index <- index + 1
        }
      }
    }
  }
}
  

keywords_overlap <- data.frame("NINR" = I(k_ninr_keys), "NICHD"= I(k_nia_keys))

write_csv(keywords_overlap, "NINR-NICHD-prevention-keyword-overlap.csv")

# Create unique keywords_overlap
unique_keywords_overlap <- keywords_overlap[!duplicated(keywords_overlap[ , c("NINR")]),]  

write_csv(unique_keywords_overlap, "NINR-NICHD-prevention-unique-keyword-overlap.csv")

length(unique(keywords_overlap$NIA))

keywords_overlap <- as.data.frame(lapply(keywords_overlap, unlist))
keywords_overlap$NINR <- as.factor(keywords_overlap$NINR)

freq_keywords <- (keywords_overlap %>% 
    group_by(NINR) %>% 
    count() %>%
    arrange(desc(n)))


write_csv(freq_keywords, "NINR-NICHD-prevention-freq-keywords.csv")
```

### Term frequency functions
```{r}
find_dtm <- function(word, word_matrix) {
  cond = 0
  freq = 0
  for (col in colnames(word_matrix)) {
    if (grepl(word, col)) {
      cond = 1
      freq <- freq + sum(word_matrix[, col])
    }
  }   
  return(list(cond=cond, freq=freq))
}

```


### Calculate term frequency with DTM

```{r}
term_frequency_pre_LDA <- k
for (row in 1:nrow(term_frequency_pre_LDA)) {
  for (col in 1:ncol(term_frequency_pre_LDA)) {
    #print(term_frequency_pre_LDA[row, col])
    term <- find_dtm(term_frequency_pre_LDA[row, col], ninr_dtm_matrix)
    if (term$cond == 1) {
      term_frequency_pre_LDA[row, col] <-  paste(term_frequency_pre_LDA[row, col], as.String(term$freq), sep= ", ")
    }
  }
}

term_frequency_pre_LDA

```
# -----------------IGNORE END--------------------------------------------------------------------------------------------------------------

## Output Application Topics

```{r}

p = inner_join(select(ninr, Project:PO.Name), d)

p2 = (
  p %>% mutate(
               T1 = round(100*T1, 2),
               T2 = round(100*T2, 2),
               T3 = round(100*T3, 2),
               T4 = round(100*T4, 2),
               T5 = round(100*T5, 2),
               T6 = round(100*T6, 2),
               T7 = round(100*T7, 2),
               T8 = round(100*T8, 2),
               T9 = round(100*T9, 2),
               T10 = round(100*T10, 2),
               T11 = round(100*T11, 2),
               T12 = round(100*T12, 2),
               T13 = round(100*T13, 2))
   # TODO: Include Title in file
   # %>% mutate(Title = paste0('=HYPERLINK(', 
   #                           '"https://apps.era.nih.gov/qvr/web/dd_abstract.cfm?ApplId=', 
   #                           AID, 
   #                           '&sourceCode=CURRENT&rcdc=Y"', 
   #                           ', "', 
   #                           Title, '")'))
)

write_csv(p2, "NINR-NIA-topics.csv")
```


## Display proportion of topics accounted for by different institutes
```{r}
# TODO: Change institute name
ninr_grants <- p2[substr(p2$Project, 1, 2) == "NR",]

nia_grants <- p2[substr(p2$Project, 1, 2) == "AG",]

hist(ninr_grants$T11)
hist(nia_grants$T11)

nia_grants %>%
  distinct(Project, .keep_all=TRUE) %>%
  ggplot(aes(x=T11)) +
    geom_histogram() +
    scale_x_continuous(breaks = round(seq(0, 100, by = 10),1)) +
    scale_y_continuous(breaks = round(seq(0, 620, by = 10),1)) 

```

```{r}
# TODO: Change threshold percentage and topic number
n = names(p)
n1 = names(select(p, Project:Topic2))
n2 = sort(n[!(n %in% n1)])

topic_list <- function(x) {
  top_list <- c()
  index = 0
  row <- x[n2]
  for (i in 1: length(row)) {
    # 100 / 13 ~ 8, so 8 * 3 = 24
    if (as.double(row[i]) >= 24) {
      top_list <- append(top_list, n2[i])
      index = index + 1
    }
  }
  return(top_list)
}

vals_ninr <- apply(ninr_grants, 1, function(x) topic_list(x))
vals_nia <- apply(nia_grants, 1, function(x) topic_list(x))

if (!is.null(vals_ninr)) {
  ninr_grants$TopicList <- vals_ninr
}

if (!is.null(vals_nia)) {
  nia_grants$TopicList <- vals_nia
}

topic_num <- "T1"

ninr_grants_topic_count = 0
for (row in 1:nrow(ninr_grants)) {
  if (topic_num %in% unlist(ninr_grants[row, "TopicList"])) {
    ninr_grants_topic_count <- ninr_grants_topic_count + 1
  }
}

nia_grants_topic_count = 0
for (row in 1:nrow(nia_grants)) {
  if (topic_num %in% unlist(nia_grants[row, "TopicList"])) {
    nia_grants_topic_count <- nia_grants_topic_count + 1
  }
}

#nia_percentage <- (nia_grants_topic_count / nrow(nia_grants)) * 100
#nia_percentage <- round(nia_percentage, 2)
#ninr_percentage <- (ninr_grants_topic_count / nrow(ninr_grants)) * 100
#ninr_percentage <- round(ninr_percentage, 2)

#pie_ninr_percentage <- round((ninr_percentage / (ninr_percentage + nia_percentage)) * 100, 2)
#pie_nia_percentage <- round((nia_percentage / (ninr_percentage + nia_percentage)) * 100, 2)

pie_ninr_percentage <- round((ninr_grants_topic_count / (ninr_grants_topic_count + nia_grants_topic_count)) * 100, 2)
pie_nia_percentage <- round((nia_grants_topic_count / (ninr_grants_topic_count + nia_grants_topic_count)) * 100, 2)
pie(c(pie_ninr_percentage, pie_nia_percentage), c(paste0("NINR (", pie_ninr_percentage, "%)"), paste0("NIA (", pie_nia_percentage, "%)")))




```

\newpage


## 2D Plots

Here we dimensionally reduce the feature vectors of each grant to two dimensions using the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm and plot each grant in the reduced 2D space.

```{r, message = FALSE, warning = FALSE, fig.height = 4.1}

dtm2 = as.matrix(dtm)
dtm3 = data.frame(dtm2)
dtm3$Project = row.names(dtm2)
dtm4 = inner_join(select(p, Project, Topic1, PO.Name), dtm3)

set.seed(123)

ts = Rtsne(as.matrix(select(dtm4, -Project, -Topic1, -PO.Name)), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2)
dts = as.data.frame(ts$Y)
dts2 = cbind(select(dtm4, Project, Topic1, PO.Name), dts)
dts2 = rename(dts2, PO.Name = PO.Name)

 (
    ggplot(dts2, aes(x = V1, y = V2, col = Topic1))
  + geom_point()
  + ggtitle("Most Likely Topic of Each Grant in Term Frequency Space")
  + theme_minimal()
  + theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4))
 )


 (
    ggplot(dts2, aes(x = V1, y = V2, col = PO.Name))
  + geom_point()
  + ggtitle("PO of Each Grant in Term Frequency Space")
  + theme_minimal()
  + theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4))
 )



set.seed(123)

ts = Rtsne(as.matrix(select(p, n2)), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2)
dts = as.data.frame(ts$Y)
dts2 = cbind(select(p, Project, Topic1, PO.Name), dts)
dts2 = rename(dts2, PO.Name = PO.Name)

 (
    ggplot(dts2, aes(x = V1, y = V2, col = Topic1)) 
  + geom_point()
  + ggtitle("Most Likely Topic of Each Grant in Topic Space")
  + theme_minimal()
  + theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4))
 )


 (
    ggplot(dts2, aes(x = V1, y = V2, col = PO.Name)) 
  + geom_point()
  + ggtitle("PO of Each Grant in Topic Space")
  + theme_minimal()
  + theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4)) 
 )
```

\newpage

## Most Likely Topics of POs

Here we look at the breakdown of most likely topic (topic with the highest percentage) by PO.

```{r, fig.height = 4.1}
p1 = (
  p %>% group_by(PO.Name)
    %>% mutate(N = n())
    %>% ungroup()
    %>% group_by(PO.Name, Topic1, N)
    %>% summarize(n = n())
    %>% ungroup()
    %>% mutate(Percentage = round(100*n/N))
)

(
    ggplot(p1, aes(x = PO.Name, y = n, fill = Topic1))
  + geom_bar(stat = "identity", width = 0.5)
  + ylab("Frequency")
  + xlab("PO")
  + ggtitle("Most Likely Topics of POs (Frequency in Portfolio)")
  #+ scale_y_continuous(labels = scales::percent)
  + theme_minimal()
  + theme(legend.position = "right",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 90, vjust = 0.4)) 
)

(
    ggplot(p1, aes(x = PO.Name, y = Percentage/100, fill = Topic1))
  + geom_bar(stat = "identity", width = 0.5)
  + ylab("Percentage")
  + xlab("PO")
  + ggtitle("Most Likely Topics of POs (Percentage of Portfolio)")
  + scale_y_continuous(labels = scales::percent)
  + theme_minimal()
  + theme(legend.position = "right",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 90, vjust = 0.4)) 
)

table(p$PO.Name)
```

## Distributions of Topics

Here we look at the distributions of the percentages of each topic in each grant, and the topic distributions of each PO's portfolio.  We use standard (Tukey) boxplots. In the boxplots, the black lines are the medians and the solid black dots are the means.

```{r, fig.height = 4.1}

p1 = (
  p %>% select(PO.Name, n2)
    %>% melt(measure.vars = n2, variable.name = "Topic", value.name = "Fraction")
    %>% arrange(PO.Name)
)

g = (
    ggplot(p1, aes(x = Topic, y = Fraction, fill = Topic))
  + geom_boxplot(width = 0.8, outlier.shape = 1)
  + stat_summary(fun.y = "mean", geom = "point", size = 0.9)
  + ylab("Percentage")
  + xlab("Topic")
  + ggtitle("Distribution of Topic Percentages")
  + scale_y_continuous(labels = scales::percent)
  + theme_minimal()
  + theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4)) 
)

print(g)

g1 = ggplot_build(g)
colors = unique(g1$data[[1]]["fill"])
colors$Topic = sort(unique(p1$Topic))

(
    ggplot(p1, aes(x = PO.Name, y = Fraction, fill = Topic))
  + geom_boxplot(width = 0.8, outlier.shape = 1)
  + stat_summary(fun.y = "mean", geom = "point", position = position_dodge(width=0.8), size = 0.9)
  + ylab("Percentage")
  + xlab("PO")
  + ggtitle("Distribution of Topic Percentages by PO")
  + scale_y_continuous(labels = scales::percent)
  + theme_minimal()
  + theme(legend.position = "right",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 90, vjust = 0.4)) 
)
```


```{r, fig.height = 4.1}
for(topic in sort(unique(p1$Topic))){
  g = (
      ggplot(filter(p1, Topic == topic), aes(x = PO.Name, y = Fraction))
    + geom_boxplot(width = 0.8, fill = colors[colors$Topic == topic, "fill"], outlier.shape = 1)
    + stat_summary(fun.y = "mean", geom = "point", size = 0.9)
    + ylab("Percentage")
    + xlab("PO")
    + scale_y_continuous(limits = c(0,1), labels = scales::percent)
    + ggtitle(paste("Distribution of Topic", topic, "by PO"))
    + theme_minimal()
    + theme(legend.position = "right",
                plot.title = element_text(hjust = 0.5),
                plot.subtitle = element_text(hjust = 0.5),
                #axis.text.x = element_text(color = s$Color),
                panel.grid.major.x = element_blank(),
                axis.text.x = element_text(angle = 90, vjust = 0.4))
  )
  
  print(g)
  cat("\n\n")
}
```






    
