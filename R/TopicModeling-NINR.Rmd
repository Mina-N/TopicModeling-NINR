---
title: "TopicModeling-NINR"
author: "Mina Narayanan"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
## Global knitr options
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(autodep = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

```

```{r, warning = FALSE, message = FALSE}
## Increase Memory Limit
invisible(utils::memory.limit(64000))
options(java.parameters = "-Xmx64000m")

## Data Frame Packages
library(dplyr)
library(stringr)
library(readr)
library(readxl)

## Data Visualization Packages
library(ggplot2)
library(Rtsne)

## Text Mining Pacakges
library(data.table)
library(Matrix)
library(text2vec)
library(tm)
library(SnowballC)
library(rARPACK)
library(ggupset)
```

## Load Data

First we load the data of NINR grants awarded in 2019.  We pull four sources of text:
\begin{itemize}
\item T - Title
\item A - Abstract Text
\item S - Specific Aims (SA) Text
\end{itemize}x
```{r, message = FALSE, cache = TRUE}
ninr = read_excel("./../data/NINR-2019-all-apps-with-RCDC.xlsx", guess_max = 1000000)


# TODO: keep PCC in ninr dataframe in the future


```

## Rename Columns
```{r}
names(ninr)[names(ninr) == 'PI Name'] <- 'PI.Name'
names(ninr)[names(ninr) == 'PO Name'] <- 'PO.Name'
names(ninr)[names(ninr) == 'RFA/PA Number'] <- 'RFA.PA.Number'                                                                                  
names(ninr)[names(ninr) == 'Stat Desc'] <- 'Stat.Desc'                                                                                          
names(ninr)[names(ninr) == 'Abstract Text (only)'] <- 'Abstract' 
names(ninr)[names(ninr) == 'SA Text'] <- 'SA.Text' 
```

Let's restrict to R01 equivalents

```{r}
#r01 = c("DP1", "DP2", "DP5", "R01", "R35", "R37", "R56", "RF1", "RL1", "U01")
```

## Text Variable

Let's create a text variable.
```{r}
text = c("Title", "Abstract", "SA.Text")
ninr <- ninr[ninr$Abstract != "-",]
ninr$Text <- ninr[[text[1]]]

if(length(text) > 1){
  for(i in 2:length(text)){
    ninr$Text <- paste(ninr$Text, ninr[[text[i]]])
    print(i)
  }
}
print(substr(ninr[1, "Text"], 1000, 11000))

ninr = select(ninr, -c("Title", "Abstract", "SA.Text"))
ninr = mutate(ninr, Text = ifelse(is.na(Text), "", Text))
ninr = filter(ninr, Text != "")

cat("Sections used:", text, "\n\n")
```

### Clean Up Text

Let's clean up the text by making everything lowercase, removing punctuation, etc.
```{r}
# Remove weird characters
ninr$Text = str_replace_all(ninr$Text, "\n", " ")
ninr$Text = str_replace_all(ninr$Text, "\r", " ")
ninr$Text = str_replace_all(ninr$Text, "'", "")
ninr$Text = str_replace_all(ninr$Text, "-", " ")

# Replace non-alpha numeric characters with a space
ninr$Text = str_replace_all(ninr$Text, "[^abcdefghijklmnopqrstuvwxyzABCDEFHIJKLMNOPQRSTUVWXZ0123456789 ]", " ")

# Put everything in lowercase
ninr$Text = tolower(ninr$Text)

# Remove a few select words and extra spaces
ninr$Text = str_replace_all(ninr$Text, "abstract", " ")
ninr$Text = str_replace_all(ninr$Text, "project", " ")
ninr$Text = str_replace_all(ninr$Text, "proposal", " ")
ninr$Text = str_replace_all(ninr$Text, "summary", " ")
ninr$Text = str_replace_all(ninr$Text, "narrative", " ")
ninr$Text = str_replace_all(ninr$Text, "background", " ")
ninr$Text = str_replace_all(ninr$Text, "supplement", " ")
ninr$Text = str_replace_all(ninr$Text, "significance", " ")
ninr$Text = str_replace_all(ninr$Text, "overall", " ")
ninr$Text = str_replace_all(ninr$Text, "title", " ")
ninr$Text = str_replace_all(ninr$Text, "specific aims", " ")
ninr$Text = str_replace_all(ninr$Text, "goal", " ")
ninr$Text = str_replace_all(ninr$Text, "research plan", " ")
ninr$Text = str_replace_all(ninr$Text, "overview", " ")
ninr$Text = str_replace_all(ninr$Text, "\\s+", " ")
ninr$Text = str_replace_all(ninr$Text, "description provided by applicant", " ")

ninr$Text = str_trim(ninr$Text, side = "both")

```

### Unique Text

Let's only keep applications with unique text.
```{r}
ninr <- ninr[!duplicated(ninr[ , c("Text")]),]

nrow(ninr)

```


## DTM

Let's build a Document-Term Matrix (DTM).
```{r}
setDT(ninr)
setkey(ninr, Project)
```


###  Vocabulary

Let's build a vocabulary that uses stemming.
```{r}
stem_tokenizer =function(x) {
  word_tokenizer(x) %>% lapply(SnowballC::wordStem, language="en") 
  }

it = itoken(ninr$Text,
            #tokenizer = word_tokenizer, 
            tokenizer = stem_tokenizer,
            ids = ninr$Project, 
            progressbar = FALSE)

stop = stopwords(kind = "SMART")
stop = str_replace_all(stop, "'", "")

vocab = create_vocabulary(it, stopwords = stop, ngram = c(1L, 2L))

cat("Number of terms in vocabulary:", dim(vocab)[1])
#vocab %>% arrange(desc(term_count)) %>% head(20)
```

Let's prune the vocabulary by only including terms that appear at least 10 times.
```{r}
vocab = prune_vocabulary(vocab,
                                term_count_min = 10)
                                #doc_count_min = 5,
                                #doc_proportion_max = 0.9,
                                #doc_proportion_min = 0.001)

cat("Number of terms in vocabulary:", dim(vocab)[1])
#vocab %>% arrange(desc(term_count)) %>% head(20)
```

### Extra Stop Words

```{r}
# TODO: Create list of custom stopwords
#paul1 = read_excel("B1_Round1_2019-12-06.xlsx", sheet = "Stopwords")
#paul2 = read_excel("B1_Round2_2019-12-06.xlsx", sheet = "Stopwords")
#paul = rbind(paul1, paul2)
#paul = paul$Stopword
#print(paul)

#vocab = filter(vocab, !(term %in% paul))
#cat("\nNumber of terms in vocabulary:", dim(vocab)[1])
```


### Build DTM

Now let's build the DTM for the training data.
```{r}
vectorizer = vocab_vectorizer(vocab)
dtm = create_dtm(it, vectorizer)

#identical(rownames(dtm), as.character(nih$AID))

cat("DTM dimensions:", dim(dtm))
```

### Write DTM to file

```{r}
m <- as.matrix(dtm)
#write.csv(m, file = 'dtm_ninr.csv')
```

## Topic Modeling

Here we use the most popular topic modeling algorithm, Latent Dirichlet Allocation (LDA).  LDA assigns a probability to each word of belonging to each topic, and then assigns a percentage of each topic to each document.  For example, if we had 2 topics, LDA would assign two percentages (which add up to 100%) to each document.  Here we plot the 20 most indicative words of each topic, and the distribution of the most likely topics of the documents.

### Lowest Perplexity

UNCOMMENT THIS CODE TO FIND THE OPTIMAL PARAMETERS AGAIN
```{r}
 as = c(0.05, 10^(-5:5))
 bs = c(0.05, 10^(-5:5))
 topics = 3:15
 
 pp = NULL
 
 for(topic in topics){
   a1 = 50/topic # Default Values
   b1 = 1/topic  # Default Values
   lda_model = LDA$new(doc_topic_prior = a1, 
                       topic_word_prior = b1,
                       n_topics = topic)
   
   set.seed(123)
   doc_topic_distr = 
        lda_model$fit_transform(x = dtm, 
                                n_iter = 1000, 
                                convergence_tol = 0.0001, 
                                n_check_convergence = 25, 
                                progressbar = FALSE,
                                verbose = FALSE)
   
   topic_word_distr = lda_model$topic_word_distribution
  
   p1 = perplexity(dtm, topic_word_distr, doc_topic_distr)
   
   print(paste(a1, b1, p1))
   
   for(a in as){
     for(b in bs){
       lda_model = LDA$new(doc_topic_prior = a, 
                           topic_word_prior = b,
                           n_topics = topic)
       
       set.seed(123)
       doc_topic_distr = 
            lda_model$fit_transform(x = dtm, 
                                    n_iter = 1000, 
                                    convergence_tol = 0.0001, 
                                    n_check_convergence = 25, 
                                    progressbar = FALSE,
                                    verbose = FALSE)
       
       topic_word_distr = lda_model$topic_word_distribution
       
       p = perplexity(dtm, topic_word_distr, doc_topic_distr)
       # TODO: Check coherence
       
       print(paste(topic, a, b, p))
       
       if(p < p1){
         a1 = a
         b1 = b
         p1 = p
       }
     }
   }
   
   pp1 = data.frame(Topics = topic, Alpha = a1, Beta = b1, Perplexity = p1)
   pp = rbind(pp, pp1)
 }
 
 cat("\n\n")
 
 pp
 write_csv(pp, "LDA_Opt_Param_NINR19.csv")
```

Let's load the optimal parameters.
```{r}
pp = read_csv("LDA_Opt_Param_NINR19.csv")
pp = data.frame(pp)

str(pp)
```

Now we choose the desired number of topic.
```{r, message = FALSE, warining = FALSE}
topic = 13

pp1 = filter(pp, Topics == topic)
a1 = pp1$Alpha
b1 = pp1$Beta
t1 = pp1$Topics

lda_model = LDA$new(doc_topic_prior = a1, 
                    topic_word_prior = b1,
                    n_topics = t1)

set.seed(123)
doc_topic_distr = 
     lda_model$fit_transform(x = dtm, 
                             n_iter = 1000, 
                             convergence_tol = 0.0001, 
                             n_check_convergence = 25, 
                             progressbar = FALSE,
                             verbose = FALSE)

topic_word_distr = lda_model$topic_word_distribution


#cat("\n\n")

d = as.matrix(doc_topic_distr)
d = data.frame(Project = row.names(d), d)
colnames(d) = str_replace_all(names(d), "X", "T")

 for(i in 1:dim(d)[1]){
    temp = names(sort(d[i, 2:dim(d)[2]], decreasing = TRUE))[1:2]           
    d[i, "Topic1"] = temp[1]
    d[i, "Topic2"] = temp[2]
 }

#d$Topic = factor(d$Topic)

l = dim(d)[2]
d = d[, c(1, l-1,l, 4:l-2)]

cat("\n\n")

k = lda_model$get_top_words(n = 20, topic_number = 1:t1, lambda = 0.25)

k

#cat("\n\n")

#perplexity(dtm, topic_word_distr, doc_topic_distr)

#cat("\n")
table(d$Topic1)

k1 = data.frame(k)
colnames(k1) = paste0("Topic", 1:t1)

#str(k1)
```

# Derive Acronyms Using Lookup Table
```{r}
acronym_lookup <- data.frame("Acronym" = c("pa", "smm", "hf", "plwh", "t2dm", "msm", "fsw", "hana", "acp", "cds", "cqa", "cci", "comt", "bpsd", "pwd", "fc", "eolpc", "pcrc", "cmc", "4_bpsd", "pes", "mcc",
                                           "hhc", "dts", "cri", "vte", "pru", "cbpr", "cbt", "trkb", "lbp", "trkb_t1", "oa", "eol"), 
                             "Derivation" = c("physical activity", "severe maternal morbidity ", "heart failure", "persons living with HIV", "type 2 diabetes mellitus", "men who have sex with men", "female
                                              sex worker", "HIV-associated non-AIDS conditions", "advance care planning", "clinical decision support", "communication quality analysis", "chronic critical
                                              illness", "catechol-O-methyltransferase", "behavioral and psychological symptoms of dementia", "persons with dementia", "fecal calprotectin", "end of life and
                                              palliative care", "Palliative Care Research Consortium", "children with medical complexity", "behavioral and psychological symptoms of dementia", "patient
                                              engagement specialist", "multiple chronic conditions", "home healthcare", "disinfection tracking system", "cardiorespiratory instability", "Venous
                                              thromboembolism", "pruritis", "community-based participatory research", "cognitive behavioral therapy", "tropomyosin-related receptor kinase type B", "lower
                                              back pain", "truncated isoform of Trkb", "osteoarthritis", "end of life"))                                                                                                                                                         
  
  
for (row in 1:nrow(k)) {
  for (col in 1:ncol(k)) {
    for (row_acro in 1:nrow(acronym_lookup)) {
      if (as.String(k[row, col]) == as.String(acronym_lookup[row_acro, 1])) {
        k[row, col] <- as.String(acronym_lookup[row_acro, 2])
      }
    }
  }
}

k
```

### Check Overlap with Kris Keywords
```{r}
kris_key <- read.delim("../data/kris_keywords.txt", header=F)
kris_overlap <- list()
index <- 1
count <- 0
flag <- 0

for (row_kris in 1:nrow(kris_key)) {
  for (row in 1:nrow(k)) {
    for (col in 1:ncol(k)) {
      k_word <- str_replace_all(k[row, col], "_", " ")
      if (grepl(tolower(as.String(k_word)), tolower(as.String(kris_key[row_kris,])), fixed=TRUE) | grepl(tolower(as.String(kris_key[row_kris,])), tolower(as.String(k_word)), fixed=TRUE)) {
        kris_overlap[index] <- tolower(as.String(kris_key[row_kris,]))
        index <- index + 1
        flag <- 1
      }
    }
  }
  if (flag == 1) {
    count <- count + 1
  }
  flag <- 0
}

length(kris_overlap)

(length(kris_overlap) / length(k)) * 100

(count / nrow(kris_key)) * 100

lapply(kris_overlap, write, "kris_overlap.txt", append=TRUE)
```

### Format PCC_key
```{r}
# Read in PCC keywords file
PCC_key <- read_excel("../data/PCC_keywords.xlsx", col_names = FALSE)

# Remove weird characters
PCC_key$...2 = str_replace_all(PCC_key$...2, "\n", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "\r", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "'", "")
PCC_key$...2 = str_replace_all(PCC_key$...2, "-", " ")

# Replace non-alpha numeric characters with a space
PCC_key$...2 = str_replace_all(PCC_key$...2, "[^abcdefghijklmnopqrstuvwxyzABCDEFHIJKLMNOPQRSTUVWXZ0123456789 ]", " ")

# Put everything in lowercase
PCC_key$...2 = tolower(PCC_key$...2)

# Remove a few select words and extra spaces
PCC_key$...2 = str_replace_all(PCC_key$...2, "abstract", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "project", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "proposal", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "summary", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "narrative", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "background", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "supplement", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "significance", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "overall", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "title", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "specific aims", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "goal", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "research plan", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "overview", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "\\s+", " ")
PCC_key$...2 = str_replace_all(PCC_key$...2, "description provided by applicant", " ")

PCC_key$...2 = str_trim(PCC_key$...2, side = "both")

```

### Check Overlap with PCC Keywords
```{r}

PCC_overlap <- list()
pcc_list <- list()
topic_keywords_list <- list()
index <- 1
count <- 0
flag <- 0
find <- 0
                 
for (row_PCC in 1:nrow(PCC_key)) {
  for (row in 1:nrow(k)) {
    for (col in 1:ncol(k)) {
      k_word <- invisible(str_replace_all(k[row, col], "_", " "))
      PCC_text <- invisible(as.list(scan(text=as.character(PCC_key[row_PCC, 2]), what='', sep=' ')))
      for (index_PCC in 1: length(PCC_text)) {
        word <- PCC_text[index_PCC]
        location <- gregexpr(pattern = tolower(as.String(k_word)), as.String(word))[[1]]
        if (location > 0) {
          print(word)
          print(k_word)
        }
        if (grepl(tolower(as.String(k_word)), tolower(as.String(PCC_key[row_PCC, 2])), fixed=TRUE) && (as.double(location / nchar(as.String(word))) < 0.5 && location > 0 && !(k_word %in% c("eh", "sm", "sca", "eh", "af", "ic", "asp")))) {
          PCC_overlap[index] <- tolower(as.String(PCC_key[row_PCC, 2]))
          pcc_list[index] <- PCC_key[row_PCC, 1]
          topic_keywords_list[index] <- k_word
          index <- index + 1
          flag <- 1
          break
        }
      }
    }
  }
  if (flag == 1) {
    count <- count + 1
  }
  flag <- 0
}

keywords_overlap <- data.frame("PCC" = I(pcc_list), "Topic_Keywords"= I(topic_keywords_list))

# Create unique keywords_overlap
unique_keywords_overlap <- keywords_overlap[!duplicated(keywords_overlap[ , c("PCC")]),]

PCC_overlap

length(unique(keywords_overlap$Topic_Keywords))

keywords_overlap <- as.data.frame(lapply(keywords_overlap, unlist))

keywords_overlap$Topic_Keywords <- as.factor(keywords_overlap$Topic_Keywords)

keywords_overlap %>% 
    group_by(Topic_Keywords) %>% 
    count() %>%
    arrange(desc(n))

```
### Create dataframe with grants, predominant topics, and RCDC concepts
```{r}
p = inner_join(select(ninr, Project:PO.Name), d)
RCDC_concepts <- p
RCDC_key <- read_excel("../data/NINR-2019-all-apps-with-RCDC.xlsx")
RCDC_merge <- merge(RCDC_key, RCDC_concepts, by="Project")
list_rcdc <- c()
index <- 1

#Convert RCDC categories column into list 

RCDC_merge$`RCDC Concepts (Scaled)` = str_replace_all(RCDC_merge$`RCDC Concepts (Scaled)`, "//", " ")

```


### Check Overlap with RCDC Categories
```{r}
topic_words <- character(nrow(RCDC_merge))
projects <- list()
rcdc <- list()
index <- 1

for (row in 1:nrow(RCDC_merge)) {
  topic_num <- as.integer(as.list(scan(text=as.character(RCDC_merge[row, 'Topic1']), what='', sep='T'))[2])
  topic_words[index] <- paste( unlist(k[, topic_num]), collapse=';')
  index <- index + 1
}

RCDC_merge$TopicKeywords <- topic_words


str(RCDC_merge[, 'RCDC Concepts (Scaled)'])
str(RCDC_merge[, 'TopicKeywords'])

# Compare keywords in RCDC Concepts (Scaled) column and TopicKeywords column

index <- 1
topic_words <- list()

for (row in 1:nrow(RCDC_merge)) {
  print(row)
  topic_keywords <- strsplit(toString(RCDC_merge[row, 'TopicKeywords']),";") 
  topic_keywords <- unlist(topic_keywords)
  RCDC_keywords <- strsplit(toString(RCDC_merge[row, 'RCDC Concepts (Scaled)']), "[[:space:]]")
  RCDC_keywords <- unlist(RCDC_keywords)
  for (index_topic_keywords in 1: length(topic_keywords)) {
    topic_word <- str_replace_all(topic_keywords[index_topic_keywords], "_", " ")
    #print(topic_word)
    for (index_RCDC_keywords in 1: length(RCDC_keywords)) {
      RCDC_word <- RCDC_keywords[index_RCDC_keywords]
      location <- gregexpr(pattern = tolower(as.String(topic_word)), as.String(RCDC_word))[[1]]
      if (grepl(tolower(as.String(topic_word)), tolower(as.String(RCDC_word)), fixed=TRUE) && (as.double(location / nchar(as.String(RCDC_word))) < 0.5 && location > 0 && !(topic_word %in% c("eh", "sm", "sca", "eh", "af", "ic", "asp")))) {
        projects[index] <- RCDC_merge[row, "Project"]
        rcdc[index] <- RCDC_word
        topic_words[index] <- topic_word
        index <- index + 1
      }
    } 
  }
}


keywords_overlap_RCDC <- data.frame("Project" = I(projects), "RCDC" = I(rcdc), "Topic_Keywords"= I(topic_words))


length(unique(keywords_overlap_RCDC$Topic_Keywords))


keywords_overlap_RCDC <- as.data.frame(lapply(keywords_overlap_RCDC, unlist))

keywords_overlap_RCDC$Project <- as.factor(keywords_overlap_RCDC$Project)

keywords_overlap_RCDC %>% 
    group_by(Topic_Keywords) %>% 
    count() %>% 
    arrange(desc(n))

```


### Term frequency functions
```{r}
find_dtm <- function(word, word_matrix) {
  cond = 0
  freq = 0
  for (col in colnames(word_matrix)) {
    if (grepl(word, col)) {
      cond = 1
      freq <- freq + sum(word_matrix[, col])
    }
  }   
  return(list(cond=cond, freq=freq))
}

```


### Calculate term frequency with DTM

```{r}
term_frequency_pre_LDA <- k
for (row in 1:nrow(term_frequency_pre_LDA)) {
  for (col in 1:ncol(term_frequency_pre_LDA)) {
    #print(term_frequency_pre_LDA[row, col])
    term <- find_dtm(term_frequency_pre_LDA[row, col], m)
    if (term$cond == 1) {
      term_frequency_pre_LDA[row, col] <-  paste(term_frequency_pre_LDA[row, col], as.String(term$freq), sep= ", ")
    }
  }
}

term_frequency_pre_LDA

```

## LDAvis

```{r}
lda_model$plot()
```

\newpage


## 2D Plots

Here we dimensionally reduce the feature vectors of each grant to two dimensions using the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm and plot each grant in the reduced 2D space.

```{r, message = FALSE, warning = FALSE, fig.height = 4.1}

n = names(p)
n1 = names(select(p, Project:Topic2))
n2 = sort(n[!(n %in% n1)])

dtm2 = as.matrix(dtm)
dtm3 = data.frame(dtm2)
dtm3$Project = row.names(dtm2)
dtm4 = inner_join(select(p, Project, Topic1, PO.Name), dtm3)

set.seed(123)

ts = Rtsne(as.matrix(select(dtm4, -Project, -Topic1, -PO.Name)), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2)
dts = as.data.frame(ts$Y)
dts2 = cbind(select(dtm4, Project, Topic1, PO.Name), dts)
dts2 = rename(dts2, PO.Name = PO.Name)

 (
    ggplot(dts2, aes(x = V1, y = V2, col = Topic1))
  + geom_point()
  + ggtitle("Most Likely Topic of Each Grant in Term Frequency Space")
  + theme_minimal()
  + theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4))
 )


 (
    ggplot(dts2, aes(x = V1, y = V2, col = PO.Name))
  + geom_point()
  + ggtitle("PO of Each Grant in Term Frequency Space")
  + theme_minimal()
  + theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4))
 )



set.seed(123)

ts = Rtsne(as.matrix(select(p, n2)), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2)
dts = as.data.frame(ts$Y)
dts2 = cbind(select(p, Project, Topic1, PO.Name), dts)
dts2 = rename(dts2, PO.Name = PO.Name)

 (
    ggplot(dts2, aes(x = V1, y = V2, col = Topic1)) 
  + geom_point()
  + ggtitle("Most Likely Topic of Each Grant in Topic Space")
  + theme_minimal()
  + theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4))
 )


 (
    ggplot(dts2, aes(x = V1, y = V2, col = PO.Name)) 
  + geom_point()
  + ggtitle("PO of Each Grant in Topic Space")
  + theme_minimal()
  + theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4)) 
 )
```

\newpage

## Most Likely Topics of POs

Here we look at the breakdown of most likely topic (topic with the highest percentage) by PO.

```{r, fig.height = 4.1}
p1 = (
  p %>% group_by(PO.Name)
    %>% mutate(N = n())
    %>% ungroup()
    %>% group_by(PO.Name, Topic1, N)
    %>% summarize(n = n())
    %>% ungroup()
    %>% mutate(Percentage = round(100*n/N))
)

(
    ggplot(p1, aes(x = PO.Name, y = n, fill = Topic1))
  + geom_bar(stat = "identity", width = 0.5)
  + ylab("Frequency")
  + xlab("PO")
  + ggtitle("Most Likely Topics of POs (Frequency in Portfolio)")
  #+ scale_y_continuous(labels = scales::percent)
  + theme_minimal()
  + theme(legend.position = "right",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 90, vjust = 0.4)) 
)

(
    ggplot(p1, aes(x = PO.Name, y = Percentage/100, fill = Topic1))
  + geom_bar(stat = "identity", width = 0.5)
  + ylab("Percentage")
  + xlab("PO")
  + ggtitle("Most Likely Topics of POs (Percentage of Portfolio)")
  + scale_y_continuous(labels = scales::percent)
  + theme_minimal()
  + theme(legend.position = "right",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 90, vjust = 0.4)) 
)

table(p$PO.Name)
```

## Distributions of Topics

Here we look at the distributions of the percentages of each topic in each grant, and the topic distributions of each PO's portfolio.  We use standard (Tukey) boxplots. In the boxplots, the black lines are the medians and the solid black dots are the means.

```{r, fig.height = 4.1}

p1 = (
  p %>% select(PO.Name, n2)
    %>% melt(measure.vars = n2, variable.name = "Topic", value.name = "Fraction")
    %>% arrange(PO.Name)
)

g = (
    ggplot(p1, aes(x = Topic, y = Fraction, fill = Topic))
  + geom_boxplot(width = 0.8, outlier.shape = 1)
  + stat_summary(fun.y = "mean", geom = "point", size = 0.9)
  + ylab("Percentage")
  + xlab("Topic")
  + ggtitle("Distribution of Topic Percentages")
  + scale_y_continuous(labels = scales::percent)
  + theme_minimal()
  + theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 0, vjust = 0.4)) 
)

print(g)

g1 = ggplot_build(g)
colors = unique(g1$data[[1]]["fill"])
colors$Topic = sort(unique(p1$Topic))

(
    ggplot(p1, aes(x = PO.Name, y = Fraction, fill = Topic))
  + geom_boxplot(width = 0.8, outlier.shape = 1)
  + stat_summary(fun.y = "mean", geom = "point", position = position_dodge(width=0.8), size = 0.9)
  + ylab("Percentage")
  + xlab("PO")
  + ggtitle("Distribution of Topic Percentages by PO")
  + scale_y_continuous(labels = scales::percent)
  + theme_minimal()
  + theme(legend.position = "right",
          plot.title = element_text(hjust = 0.5),
          axis.text.x = element_text(angle = 90, vjust = 0.4)) 
)
```


```{r, fig.height = 4.1}
for(topic in sort(unique(p1$Topic))){
  g = (
      ggplot(filter(p1, Topic == topic), aes(x = PO.Name, y = Fraction))
    + geom_boxplot(width = 0.8, fill = colors[colors$Topic == topic, "fill"], outlier.shape = 1)
    + stat_summary(fun.y = "mean", geom = "point", size = 0.9)
    + ylab("Percentage")
    + xlab("PO")
    + scale_y_continuous(limits = c(0,1), labels = scales::percent)
    + ggtitle(paste("Distribution of Topic", topic, "by PO"))
    + theme_minimal()
    + theme(legend.position = "right",
                plot.title = element_text(hjust = 0.5),
                plot.subtitle = element_text(hjust = 0.5),
                #axis.text.x = element_text(color = s$Color),
                panel.grid.major.x = element_blank(),
                axis.text.x = element_text(angle = 90, vjust = 0.4))
  )
  
  print(g)
  cat("\n\n")
}
```

## Output Application Topics

```{r}
p2 = (
  p %>% mutate(
               T1 = round(100*T1, 2),
               T2 = round(100*T2, 2),
               T3 = round(100*T3, 2),
               T4 = round(100*T4, 2),
               T5 = round(100*T5, 2),
               T6 = round(100*T6, 2),
               T7 = round(100*T7, 2),
               T8 = round(100*T8, 2),
               T9 = round(100*T9, 2),
               T10 = round(100*T10, 2),
               T11 = round(100*T11, 2),
               T12 = round(100*T12, 2),
               T13 = round(100*T13, 2))
   # TODO: Include Title in file
   # %>% mutate(Title = paste0('=HYPERLINK(', 
   #                           '"https://apps.era.nih.gov/qvr/web/dd_abstract.cfm?ApplId=', 
   #                           AID, 
   #                           '&sourceCode=CURRENT&rcdc=Y"', 
   #                           ', "', 
   #                           Title, '")'))
)

write_csv(p2, "NINR-2019-overlap-topics.csv")
```

## ggupset plots

```{r}

topic_list <- function(x) {
  top_list <- c()
  index = 0
  row <- x[n2]
  for (i in 1: length(row)) {
    if (as.double(row[i]) >= 33.33) {
      top_list <- append(top_list, n2[i])
      index = index + 1
    }
  }
  return(top_list)
}

vals <- apply(p2, 1, function(x) topic_list(x))

p2$FY <- ninr$FY

if (!is.null(vals)) {
  p2$TopicList <- vals
  p2_exclude <- p2[p2$TopicList == 'NULL',]
  
  p2 %>%
    distinct(Project, .keep_all=TRUE) %>%
    ggplot(aes(x=TopicList)) +
      geom_bar() +
      scale_x_continuous(breaks = round(seq(0, 200, by = 50),1)) +
      scale_y_continuous(breaks = round(seq(0, 200, by = 50),1)) + 
      scale_x_upset(order_by = "freq")
  
  #print('Number of grants before applying threshold: ')
  #print(nrow(p2))
  #print('Number of grants after applying threshold: ')
  #print(nrow(p2) - nrow(p2_exclude))
}

```






    
